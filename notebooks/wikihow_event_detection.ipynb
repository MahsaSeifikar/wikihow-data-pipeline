{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Detection on WikiHow Website\n",
    "\n",
    "In this notebook, We extract just a random day of the crawled data, preprocess and extract events using **KMEANS** algorithm. \n",
    "\n",
    "\n",
    "For more information about various steps of this notebook you can see the following links:\n",
    "\n",
    "1. https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a?gi=3ae959754bd9 \n",
    "\n",
    "2. https://medium.com/@adriensieg/text-similarities-da019229c894 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.dirname(os.getcwd())+'/data/processed/trend/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract  a random day of the collected data from preprocessed folder\n",
    "file = random.choice(os.listdir(DATA_PATH))\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, file), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_crawled</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>date_modified</th>\n",
       "      <th>n_views</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>mean_votes</th>\n",
       "      <th>description</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>How to Make Hand Sanitizer: 8 Steps (with Pict...</td>\n",
       "      <td>2008-03-09</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1,970,611</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>\\nWashing your hands with soap is better, but ...</td>\n",
       "      <td>[{'@type': 'HowToSection', 'name': 'Alcohol-Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>3 Ways to Play UNO - wikiHow</td>\n",
       "      <td>2007-07-28</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>2,886,397</td>\n",
       "      <td>206</td>\n",
       "      <td>74</td>\n",
       "      <td>\\nIf you're looking for a fun card game to pla...</td>\n",
       "      <td>[{'@type': 'HowToSection', 'name': 'Jumping in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>3 Easy Ways to Stay Productive While Working f...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>6,009</td>\n",
       "      <td>8</td>\n",
       "      <td>95</td>\n",
       "      <td>\\nWorking from home offers a wonderful level o...</td>\n",
       "      <td>[{'@type': 'HowToSection', 'name': 'Organizing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>How to Make Disinfectant Wipes: 14 Steps (with...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>7,736</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>\\nDisposable disinfecting wipes offer a quick ...</td>\n",
       "      <td>[{'@type': 'HowToSection', 'name': 'Creating S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>4 Easy Ways to Take the U.S. Census - wikiHow</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>7,195</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>\\nEvery 10 years, the U.S. government carries ...</td>\n",
       "      <td>[{'@type': 'HowToSection', 'name': 'Responding...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_crawled                                              title  \\\n",
       "0   2020-03-30  How to Make Hand Sanitizer: 8 Steps (with Pict...   \n",
       "1   2020-03-30                       3 Ways to Play UNO - wikiHow   \n",
       "2   2020-03-30  3 Easy Ways to Stay Productive While Working f...   \n",
       "3   2020-03-30  How to Make Disinfectant Wipes: 14 Steps (with...   \n",
       "4   2020-03-30      4 Easy Ways to Take the U.S. Census - wikiHow   \n",
       "\n",
       "  date_published date_modified    n_views  n_votes  mean_votes  \\\n",
       "0     2008-03-09    2020-03-20  1,970,611       95          91   \n",
       "1     2007-07-28    2020-03-27  2,886,397      206          74   \n",
       "2     2020-03-13    2020-03-26      6,009        8          95   \n",
       "3     2020-03-12    2020-03-25      7,736       14         100   \n",
       "4     2020-03-06    2020-03-23      7,195        5         100   \n",
       "\n",
       "                                         description  \\\n",
       "0  \\nWashing your hands with soap is better, but ...   \n",
       "1  \\nIf you're looking for a fun card game to pla...   \n",
       "2  \\nWorking from home offers a wonderful level o...   \n",
       "3  \\nDisposable disinfecting wipes offer a quick ...   \n",
       "4  \\nEvery 10 years, the U.S. government carries ...   \n",
       "\n",
       "                                               steps  \n",
       "0  [{'@type': 'HowToSection', 'name': 'Alcohol-Ba...  \n",
       "1  [{'@type': 'HowToSection', 'name': 'Jumping in...  \n",
       "2  [{'@type': 'HowToSection', 'name': 'Organizing...  \n",
       "3  [{'@type': 'HowToSection', 'name': 'Creating S...  \n",
       "4  [{'@type': 'HowToSection', 'name': 'Responding...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "In this section we implement the following steps to process text. After that we use _TFIDF_ to extract features. \n",
    "1. Tokenize word.\n",
    "2. Convert upper-case to lower-case.\n",
    "3. Remove punctuation.\n",
    "4. Remove non-alphabetic words.\n",
    "5. Remove stopwords.\n",
    "6. Stem the words.\n",
    "\n",
    "__NOTE:__ In **NLTK** library you should download stopwords and punctuation files as follows:\n",
    "\n",
    "```\n",
    ">>> import nltk\n",
    ">>> nltk.download('punkt')\n",
    ">>> nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    # 0- concat title and description since we use just them \n",
    "    df['text'] = df['title'] + df['description']\n",
    "\n",
    "    # 1. tokenize description and description\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "    # 2- convet upper case to lower case\n",
    "    df['text'] = df['text'].apply(lambda token: [w.lower() for w in token])\n",
    "\n",
    "    # 3- remove punctuation\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    df['text'] = df['text'].apply(lambda token: [w.translate(table) for w in token])\n",
    "\n",
    "    # 4- remian just alphabet tockens\n",
    "    df['text'] = df['text'].apply(lambda token: [w for w in token if w.isalpha()])\n",
    "\n",
    "    # 5- remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update({'make', 'steps', 'step', 'easy', 'ways', 'way', 'wikihow', 'also'})\n",
    "    df['text'] = df['text'].apply(lambda token: [w for w in token if not w in stop_words]) \n",
    "\n",
    "    # 6- Normalize text \n",
    "    porter = PorterStemmer()\n",
    "    df['text'] = df['text'].apply(lambda token: [porter.stem(w) for w in token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    # TFidf identifier\n",
    "    tfidf = TfidfVectorizer()\n",
    "    df['text'] = df['text'].apply(lambda token: \" \".join(token))\n",
    "    features_vector = tfidf.fit_transform(df['text'])\n",
    "    return pd.DataFrame(features_vector.toarray(),\n",
    "                               columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering \n",
    "In this section first, we use _SILHOUETTE SCORE_ to find the optimal number of the cluster then us _KMEANS_ algorithm to find the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose optimum number of cluser\n",
    "def get_cluster_number(df):\n",
    "    score_list = []\n",
    "    for c in range(2, 6):\n",
    "        kmeans = KMeans(n_clusters=c, random_state=0).fit(features_df)\n",
    "        score_list.append((c, silhouette_score(features_df, kmeans.labels_)))\n",
    "    score_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return score_list[0][0]\n",
    "\n",
    "def get_clusters_words(cls, df):\n",
    "    cluster_words = []\n",
    "    for i in range(cls.n_clusters):\n",
    "        cluster_words.append(' '.join(df.loc[df['class_label']==i, 'text']))\n",
    "        \n",
    "    return cluster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/envs/data_engineering/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5fb204269869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_cluster_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-023be2f66486>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# TFidf identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     return pd.DataFrame(features_vector.toarray(),\n",
      "\u001b[0;32m~/envs/data_engineering/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/data_engineering/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=get_cluster_number(features_df), random_state=0).fit(features_df)\n",
    "df['class_label'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clouds = []\n",
    "for c in range(kmeans.n_clusters):\n",
    "    words = ' '.join(df[df['class_label']==c]['text'].tolist())\n",
    "    word_clouds.append(WordCloud(width=600,\n",
    "                                 height=600,\n",
    "                                 background_color='white').generate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(kmeans.n_clusters, 1, figsize= (20, 20))\n",
    "\n",
    "for c in range(kmeans.n_clusters):\n",
    "    axes[c].title.set_text('Cluster Number: %d' %c)\n",
    "    axes[c].imshow(word_clouds[c]) \n",
    "    axes[c].axis(\"off\") \n",
    "    fig.tight_layout(pad=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cossine_similarity(c1, c2):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform([c1, c2])\n",
    "    return cosine_similarity(tfidf_matrix).flatten()[1]\n",
    "    \n",
    "def map_clusters(cluster1, cluster2):\n",
    "    dic = {}\n",
    "    for i_c1 in range(len(cluster1)):\n",
    "        for i_c2 in range(len(cluster2)):\n",
    "            if compute_cossine_similarity(cluster1[i_c1], cluster2[i_c2]) > 0.5:\n",
    "                if i_c1 in dic:\n",
    "                    dic[i_c1].append(i_c2)\n",
    "                else:\n",
    "                    dic[i_c1]=[i_c2]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort path based on modification time\n",
    "paths = sorted(Path(DATA_PATH).iterdir(), key=os.path.getmtime)\n",
    "previous_clusters_words = []\n",
    "for file in paths:\n",
    "    print(str(file).split('/')[-1])\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    clean_dataframe(df)\n",
    "    features_df = get_features(df)\n",
    "    kmeans = KMeans(n_clusters=get_cluster_number(features_df), random_state=0).fit(features_df)\n",
    "    df['class_label'] = kmeans.labels_\n",
    "    current_clusters_words = get_clusters_words(kmeans, df)\n",
    "    if previous_clusters_words:\n",
    "        print(map_clusters(current_clusters_words, \n",
    "                          previous_clusters_words))\n",
    "    previous_clusters_words = current_clusters_words\n",
    "    print('=======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Engineering",
   "language": "python",
   "name": "data_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
